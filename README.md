# SKNet-Augmented UNet3+ with Depthwise Separable Convolutions for Precise Liquid Level Segmentation in Volumetric Flasks
This code constitutes part of the project source code and datasets made publicly available for submission to The Visual Computer.The code has been optimised based on UNet3+, achieving precise segmentation of the volumetric flask liquid level.
![投稿摘要图片](https://github.com/user-attachments/assets/84bbf5ca-d808-4c16-a54e-785bc8482109)

### Network
The liquid level in volumetric flasks appears unstable in camera images due to the transparent and reflective nature of the flask material, and is easily affected by background interference. Consequently, visual liquid level recognition is susceptible to variations in liquid position and flask material. To address these issues, this paper incorporates an SKNet module following the feature fusion section of UNet3+, thereby enhancing the extraction capability of liquid level features. Furthermore, during decoder fusion of isometric feature maps, the original convolutional layers are replaced with depth-separable convolutional layers. This approach significantly reduces the number of parameters whilst incurring minimal loss in model accuracy, thereby accelerating model training speed. 
<img width="849" height="273" alt="image" src="https://github.com/user-attachments/assets/b618e614-e017-4df1-bb0c-35767949aabf" />

### Training

```console
> python train.py -h
usage: train.py [-h] [--epochs E] [--batch-size B] [--learning-rate LR]
                [--load LOAD] [--scale SCALE] [--validation VAL] [--amp]

Train the UNet on images and target masks

optional arguments:
  -h, --help            show this help message and exit
  --epochs E, -e E      Number of epochs
  --batch-size B, -b B  Batch size
  --learning-rate LR, -l LR
                        Learning rate
  --load LOAD, -f LOAD  Load model from a .pth file
  --scale SCALE, -s SCALE
                        Downscaling factor of the images
  --validation VAL, -v VAL
                        Percent of the data that is used as validation (0-100)
  --amp                 Use mixed precision
```

By default, the `scale` is 0.5, so if you wish to obtain better results (but use more memory), set it to 1.

Automatic mixed precision is also available with the `--amp` flag. [Mixed precision](https://arxiv.org/abs/1710.03740) allows the model to use less memory and to be faster on recent GPUs by using FP16 arithmetic. Enabling AMP is recommended.


### Prediction

After training your model and saving it to `MODEL.pth`, you can easily test the output masks on your images via the CLI.

To predict a single image and save it:

`python predict.py -i image.jpg -o output.jpg`

To predict a multiple images and show them without saving them:

`python predict.py -i image1.jpg image2.jpg --viz --no-save`

```console
> python predict.py -h
usage: predict.py [-h] [--model FILE] --input INPUT [INPUT ...] 
                  [--output INPUT [INPUT ...]] [--viz] [--no-save]
                  [--mask-threshold MASK_THRESHOLD] [--scale SCALE]

Predict masks from input images

optional arguments:
  -h, --help            show this help message and exit
  --model FILE, -m FILE
                        Specify the file in which the model is stored
  --input INPUT [INPUT ...], -i INPUT [INPUT ...]
                        Filenames of input images
  --output INPUT [INPUT ...], -o INPUT [INPUT ...]
                        Filenames of output images
  --viz, -v             Visualize the images as they are processed
  --no-save, -n         Do not save the output masks
  --mask-threshold MASK_THRESHOLD, -t MASK_THRESHOLD
                        Minimum probability value to consider a mask pixel white
  --scale SCALE, -s SCALE
                        Scale factor for the input images
```
You can specify which model file to use with `--model MODEL.pth`.

## Weights & Biases

The training progress can be visualized in real-time using [Weights & Biases](https://wandb.ai/).  Loss curves, validation curves, weights and gradient histograms, as well as predicted masks are logged to the platform.

When launching a training, a link will be printed in the console. Click on it to go to your dashboard. If you have an existing W&B account, you can link it
 by setting the `WANDB_API_KEY` environment variable. If not, it will create an anonymous run which is automatically deleted after 7 days.



## Data
The Carvana data is available on the dataset folder。

